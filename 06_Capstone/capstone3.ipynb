{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-29T15:09:54.081095",
     "start_time": "2016-12-29T15:09:54.071306"
    }
   },
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project - Street View House Numbers\n",
    "Thomas Wieczorek\n",
    "January 1st, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:52:34.166643",
     "start_time": "2017-01-18T18:52:33.852711"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:52:38.376218",
     "start_time": "2017-01-18T18:52:34.169335"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import hashlib\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from scipy.misc import imresize\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:52:38.393032",
     "start_time": "2017-01-18T18:52:38.378483"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG_MODUS = False\n",
    "LEARN_MODUS = True\n",
    "DEVICE_NAME = \"/cpu:0\" #For Training with GPU change to: \"/gpu:0\"\n",
    "\n",
    "train_file = \"data/train_32x32.mat\"\n",
    "test_file = \"data/test_32x32.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:52:39.109417",
     "start_time": "2017-01-18T18:52:38.395797"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/lab.ipynb\n",
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    :param url: URL to file\n",
    "    :param file: Local file path\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downloading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# Download the training and test dataset.\n",
    "download('http://ufldl.stanford.edu/housenumbers/train_32x32.mat', train_file)\n",
    "download('http://ufldl.stanford.edu/housenumbers/test_32x32.mat', test_file)\n",
    "\n",
    "# Make sure the files aren't corrupted\n",
    "assert hashlib.md5(open(train_file, 'rb').read()).hexdigest() == 'e26dedcc434d2e4c54c9b2d4a06d8373',\\\n",
    "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
    "assert hashlib.md5(open(test_file, 'rb').read()).hexdigest() == 'eb5a983be6a315427106f1b164d9cef3',\\\n",
    "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
    "\n",
    "# Wait until you see that all files have been downloaded.\n",
    "print('All files downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:52:43.105994",
     "start_time": "2017-01-18T18:52:39.112472"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#************** Extract **************#\n",
    "\n",
    "train_dataset = sio.loadmat(train_file)\n",
    "test_dataset = sio.loadmat(test_file)\n",
    "\n",
    "X_train, y_train = train_dataset['X'], train_dataset['y']\n",
    "X_test, y_test = test_dataset['X'], test_dataset['y']\n",
    "\n",
    "#*********** Preprocessing ***********#\n",
    "\n",
    "#Put the images from the last to the first index\n",
    "X_train = np.transpose(X_train, (3,0,1,2))\n",
    "X_test = np.transpose(X_test, (3,0,1,2))\n",
    "\n",
    "#Reshape from (xxx,1) to (xxx)\n",
    "y_train = np.reshape(y_train, (y_train.shape[0]))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0]))\n",
    "\n",
    "# House Numbers with '0' were marked as '10'. \n",
    "# Changing every 10 to 0\n",
    "y_train[np.where(y_train==10)] = 0\n",
    "y_test[np.where(y_test==10)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:52:43.151007",
     "start_time": "2017-01-18T18:52:43.111736"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"X_train.shape: \\t{}\".format(X_train.shape))\n",
    "print(\"y_train.shape: \\t{}\".format(y_train.shape))\n",
    "print(\"X_test.shape:  \\t{}\".format(X_test.shape))\n",
    "print(\"y_test.shape:  \\t{}\".format(y_test.shape))\n",
    "\n",
    "#Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "#Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "#Shape of a house number image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# Amount of unique classes/labels there are in the dataset.\n",
    "n_classes = y_train.ptp() + 1 \n",
    "\n",
    "print(\"\\nNumber of training examples =\\t\", n_train)\n",
    "print(\"Number of testing examples =\\t\", n_test)\n",
    "print(\"Image data shape =\\t\\t\", image_shape)\n",
    "print(\"Number of classes =\\t\\t\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:52:43.159730",
     "start_time": "2017-01-18T18:52:43.153619"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
    "is_features_normal = False\n",
    "is_labels_encod = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:52:44.307985",
     "start_time": "2017-01-18T18:52:43.162348"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Histogram of house number occurrences\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(y_train, np.arange(n_classes+1) - 0.5) #+1 and -0.5 is for centering labels\n",
    "plt.xticks(range(n_classes))\n",
    "plt.xlim([-1, n_classes])\n",
    "plt.title(\"Number of images in every class\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.xlabel(\"House number\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.wikiwand.com/en/Benford's_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:14.682198",
     "start_time": "2017-01-18T18:52:44.310279"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "def visualize_class(classId, num_images = 8):\n",
    "    \"\"\"Display a few signs of a single class\n",
    "    Keyword arguments:\n",
    "    classID    -- Class ID of the sign\n",
    "    num_images -- Number of images shown\n",
    "    \"\"\"\n",
    "    \n",
    "    examples_train = X_train[np.where(y_train==classId)[0]]\n",
    "    # pick random index\n",
    "    idx = np.random.choice(len(examples_train), num_images)\n",
    "    \n",
    "    f, ax = plt.subplots(1, num_images, figsize=(16,3))\n",
    "    f.suptitle(\"{} .\".format(classId), fontsize='xx-large')\n",
    "    \n",
    "    for i in range(num_images):     \n",
    "        ax[i].imshow(examples_train[idx[i]])\n",
    "        ax[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for c in range(n_classes):\n",
    "    if not DEBUG_MODUS:\n",
    "        visualize_class(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:31.253606",
     "start_time": "2017-01-18T18:53:14.689913"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(datetime.now())\n",
    "\n",
    "def plot_images(offset = 0, num_images = 5):\n",
    "    \"\"\"Plot signs from the offset.\"\"\"\n",
    "    print(\"Plot {} images from the offset {}\".format(num_images, offset))\n",
    "    f, ax = plt.subplots(num_images, num_images)\n",
    "    for a in range(num_images):\n",
    "        for i in range(num_images):\n",
    "            id = offset+i+(a*num_images)\n",
    "            ax[a][i].imshow(X_train[id])\n",
    "            ax[a][i].set_title(\"#{}\".format(id))\n",
    "            ax[a][i].axis('off')\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "def plot_random_train(offset = 0, num_images = 5):\n",
    "    \"\"\"Plot random signs.\"\"\"\n",
    "    print(\"Plot {} *random* images\".format(num_images, offset))\n",
    "    f, ax = plt.subplots(num_images, num_images)\n",
    "    for a in range(num_images):\n",
    "        for i in range(num_images):\n",
    "            id = offset\n",
    "            ax[a][i].imshow(X_train[id])\n",
    "            ax[a][i].set_title(\"#{}\".format(id))\n",
    "            ax[a][i].axis('off')\n",
    "            offset = random.randint(0,len(X_train))\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "if not DEBUG_MODUS:\n",
    "    plot_images(offset=22347)\n",
    "    plot_random_train()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no abnormalities or characteristics found, however some images are very blurry, and it is even hard for humans to identify them.\n",
    "![image alt >](res/Blurry.png)\n",
    "<center>_6. Example of Blurry Image_</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:39.025131",
     "start_time": "2017-01-18T18:53:31.256319"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize_YUV(img):\n",
    "    yuv = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    yuv[:,:,0] = cv2.equalizeHist(yuv[:,:,0])\n",
    "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB) \n",
    "\n",
    "# data preperation: normalize\n",
    "if not is_features_normal:       \n",
    "    ## Normalize Training Set\n",
    "    for i, pic in enumerate(X_train):\n",
    "        X_train[i] = normalize_YUV(X_train[i])\n",
    "\n",
    "    ## Normalize Test Set\n",
    "    for i, pic in enumerate(X_test):\n",
    "        X_test[i] = normalize_YUV(X_test[i])\n",
    "    \n",
    "    #Only normalize once\n",
    "    is_features_normal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:39.082375",
     "start_time": "2017-01-18T18:53:39.027889"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "if not is_labels_encod:\n",
    "    # Turn labels into numbers and apply One-Hot Encoding\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "    \n",
    "    #Only One Hot Encode once\n",
    "    is_labels_encod = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:39.128070",
     "start_time": "2017-01-18T18:53:39.088913"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check if One-Hot Encoding works\n",
    "print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "for i in range(5):\n",
    "    a=random.randint(0,n_train)\n",
    "    print(\"index = {}\\n{}\".format(a, y_train[a]))\n",
    "    \n",
    "print(\"\\n\\ny_test.shape: {}\".format(y_test.shape))\n",
    "for i in range(5):\n",
    "    a=random.randint(0,n_test)\n",
    "    print(\"index = {}\\n{}\".format(a, y_test[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:39.143170",
     "start_time": "2017-01-18T18:53:39.133489"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One Hot to Classnumber\n",
    "y_test_classes = np.argmax(y_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:39.160187",
     "start_time": "2017-01-18T18:53:39.148221"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_test_classes[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split in Train, Val and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:42.423301",
     "start_time": "2017-01-18T18:53:39.164931"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, stratify = y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:42.438731",
     "start_time": "2017-01-18T18:53:42.429301"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train size: \\t\\t{}\\nValidiation size: \\t{}\\nTest size: \\t\\t{}\".format(\n",
    "        X_train.shape, X_val.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:42.471436",
     "start_time": "2017-01-18T18:53:42.461654"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:42.609554",
     "start_time": "2017-01-18T18:53:42.477706"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeNet(x, keep_prob):\n",
    "\n",
    "    # 28x28x6\n",
    "    conv1_W = tf.Variable(tf.truncated_normal([5, 5, 3, 6], stddev = 0.01))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # 14x14x6\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # 10x10x16\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16),stddev = 0.01))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # 5x5x16\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')   \n",
    "\n",
    "    # Flatten\n",
    "    fc1 = flatten(conv2)\n",
    "    # (5 * 5 * 16, 120)\n",
    "\n",
    "    fc1_W = tf.Variable(tf.truncated_normal([400,120],stddev = 0.01))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1 = tf.matmul(fc1, fc1_W) + fc1_b\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    #Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(120, n_classes), stddev = 0.01))\n",
    "    fc2_b = tf.Variable(tf.zeros(n_classes))\n",
    "    return tf.matmul(fc1, fc2_W) + fc2_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:44.172168",
     "start_time": "2017-01-18T18:53:42.616439"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device(DEVICE_NAME):\n",
    "    # Data consists of 32x32x3 images\n",
    "    x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "\n",
    "    # Classify over 43 signs\n",
    "    y = tf.placeholder(tf.float32, (None, n_classes))\n",
    "\n",
    "    #Keep probability for Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    #Call LeNet\n",
    "    fc2 = LeNet(x, keep_prob)\n",
    "\n",
    "    # Loss\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(fc2, y))\n",
    "\n",
    "    # Optimizer: AdamOptimizer\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.0002)\n",
    "    train_op = opt.minimize(loss_op)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(fc2, 1), tf.argmax(y, 1))\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    predictions = tf.cast(tf.argmax(fc2,1), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:44.212143",
     "start_time": "2017-01-18T18:53:44.176763"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_data(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Given a dataset as input returns the loss and accuracy.\n",
    "    \"\"\"\n",
    "    # If dataset.num_examples is not divisible by BATCH_SIZE\n",
    "    # the remainder will be discarded.\n",
    "    # Ex: If BATCH_SIZE is 64 and training set has 55000 examples\n",
    "    # steps_per_epoch = 55000 // 64 = 859\n",
    "    # num_examples = 859 * 64 = 54976\n",
    "    #\n",
    "    # So in that case we go over 54976 examples instead of 55000.\n",
    "    steps_per_epoch = len(X_data) // BATCH_SIZE\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_start = step * BATCH_SIZE\n",
    "        batch_end = (step + 1) * BATCH_SIZE\n",
    "        batch_x = X_data[batch_start:batch_end] \n",
    "        batch_y = y_data[batch_start:batch_end]\n",
    "        \n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_acc += (acc * batch_x.shape[0])\n",
    "        total_loss += (loss * batch_x.shape[0])\n",
    "        \n",
    "    return total_loss/num_examples, total_acc/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T18:53:44.257568",
     "start_time": "2017-01-18T18:53:44.214894"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_in_batches(X_data):\n",
    "    steps_per_epoch = len(X_data) // BATCH_SIZE\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    total_acc, total_loss = 0, 0\n",
    "    preds = np.ndarray(shape=(len(X_data)), dtype=np.int32)\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_start = step * BATCH_SIZE\n",
    "        batch_end = (step + 1) * BATCH_SIZE\n",
    "        batch_x = X_data[batch_start:batch_end] \n",
    "        \n",
    "        preds[batch_start:batch_end] = sess.run(predictions, feed_dict={x: batch_x, keep_prob: 1.0})\n",
    "    \n",
    "    #Last part which is too small for BATCH_SIZE\n",
    "    batch_x = X_data[batch_end+1:len(X_data)-1]\n",
    "    if not len(batch_x)==0: \n",
    "        preds[batch_end+1:len(X_data)-1] = sess.run(predictions, feed_dict={x: batch_x, keep_prob: 1.0})\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T19:03:16.001590",
     "start_time": "2017-01-18T18:53:44.261851"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if LEARN_MODUS:\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "            num_examples = steps_per_epoch * BATCH_SIZE\n",
    "\n",
    "            # Train model\n",
    "            for i in range(EPOCHS):\n",
    "                for step in range(steps_per_epoch):\n",
    "                    #Calculate next Batch\n",
    "                    batch_start = step * BATCH_SIZE\n",
    "                    batch_end = (step + 1) * BATCH_SIZE\n",
    "                    batch_x = X_train[batch_start:batch_end] \n",
    "                    batch_y = y_train[batch_start:batch_end]\n",
    "                    \n",
    "                    #Run Training\n",
    "                    loss = sess.run(train_op, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "\n",
    "                #Calculate Training Loss and Accuracy\n",
    "                train_loss, train_acc = eval_data(X_train, y_train)\n",
    "                print(\"EPOCH {} ...\".format(i+1))\n",
    "                print(\"Training loss = {:.3f}\".format(train_loss))\n",
    "                print(\"Training accuracy = {:.3f}\".format(train_acc))\n",
    "                train_losses.append(train_loss)\n",
    "                train_accuracies.append(train_acc)\n",
    "                \n",
    "                #Calculate Validation Loss and Accuracy\n",
    "                val_loss, val_acc = eval_data(X_val, y_val)\n",
    "                print(\"EPOCH {} ...\".format(i+1))\n",
    "                print(\"Validation loss = {:.3f}\".format(val_loss))\n",
    "                print(\"Validation accuracy = {:.3f}\".format(val_acc))\n",
    "                val_losses.append(val_loss)\n",
    "                val_accuracies.append(val_acc)\n",
    "                \n",
    "                #Calculate Test Loss and Accuracy (Should not be done, because of survivor bias)\n",
    "                test_loss, test_acc = eval_data(X_test, y_test)\n",
    "                print(\"EPOCH {} ...\".format(i+1))\n",
    "                print(\"Test loss = {:.3f}\".format(test_loss))\n",
    "                print(\"Test accuracy = {:.3f}\".format(test_acc))\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracies.append(test_acc)\n",
    "            try:\n",
    "                saver\n",
    "            except NameError:\n",
    "                saver = tf.train.Saver()\n",
    "            saver.save(sess, 'foo')\n",
    "            print(\"Model saved\")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T19:03:16.002957",
     "start_time": "2017-01-18T17:52:33.702Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(EPOCHS), val_accuracies, 'r-',\n",
    "        range(EPOCHS), train_accuracies, 'b-')\n",
    "        range(EPOCHS), test_accuracies, 'g-')\n",
    "        #range(EPOCHS), (list(np.array(val_accuracies) - np.array(train_accuracies))), 'g-')\n",
    "#plt.plot(train_accuracies)\n",
    "plt.legend([\"validation accuracies\",\"train accuracies, test accuracies\"],loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T19:03:16.003671",
     "start_time": "2017-01-18T17:52:33.707Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Training:\\n\")\n",
    "for i, a in enumerate(train_accuracies):\n",
    "    print(\"{}: {:.4f}\".format(i,a))\n",
    "    \n",
    "print(\"Validation:\\n\")\n",
    "for i, a in enumerate(val_accuracies):\n",
    "    print(\"{}: {:.4f}\".format(i,a))\n",
    "    \n",
    "print(\"Test:\\n\")\n",
    "for i, a in enumerate(test_accuracies):\n",
    "    print(\"{}: {:.4f}\".format(i,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T19:03:16.004223",
     "start_time": "2017-01-18T17:52:33.714Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    saver\n",
    "except NameError:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:   \n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.import_meta_graph('foo.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    test_loss, test_acc = eval_data(X_test, y_test)\n",
    "    print(\"Test loss = {:.3f}\".format(test_loss))\n",
    "    print(\"Test accuracy = {:.3f}\".format(test_acc))\n",
    "\n",
    "    predicted_classes = np.array(prediction_in_batches(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T19:03:16.004670",
     "start_time": "2017-01-18T17:52:33.717Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    if not normalize:\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T19:03:16.005036",
     "start_time": "2017-01-18T17:52:33.720Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Confusion Matrix\n",
    "cnf_matrix = confusion_matrix(y_test_classes, predicted_classes)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "#Plot with Number\n",
    "plot_confusion_matrix(cnf_matrix,classes=range(n_classes),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T19:03:16.005330",
     "start_time": "2017-01-18T17:52:33.723Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_five = [142, 1542, 2842, 3024, 4442]\n",
    "test_five = X_test[index_five]\n",
    "sess = tf.Session()\n",
    "\n",
    "predicted_classes = []\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    new_saver = tf.train.import_meta_graph('foo.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "       \n",
    "    predicted_classes = sess.run(predictions, feed_dict={x: test_five, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T19:03:16.005852",
     "start_time": "2017-01-18T17:52:33.729Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(len(index_five), 1, figsize=(12.0, 25.0))\n",
    "f.subplots_adjust(hspace=.5)\n",
    "for i, idx in enumerate(index_five):    \n",
    "    #Plot Image\n",
    "    ax[i].imshow(X_test[idx])\n",
    "    ax[i].set_title(\"{} \\n Predicted Class: {} \\n Real Class: {}\".format(index_five[i], predicted_classes[i], y_test_classes[idx]))\n",
    " \n",
    " \n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "**Before submitting, ask yourself. . .**\n",
    "\n",
    "- Does the project report you’ve written follow a well-organized structure similar to that of the project template?\n",
    "- Is each section (particularly **Analysis** and **Methodology**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?\n",
    "- Would the intended audience of your project be able to understand your analysis, methods, and results?\n",
    "- Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?\n",
    "- Are all the resources used for this project correctly cited and referenced?\n",
    "- Is the code that implements your solution easily readable and properly commented?\n",
    "- Does the code execute without error and produce results similar to those reported?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND-TensorFlow-Lab]",
   "language": "python",
   "name": "conda-env-CarND-TensorFlow-Lab-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
